{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748a18b0-3ac8-4a20-bdcf-e098cc55f224",
   "metadata": {},
   "source": [
    "## Part 1: Camera Calibration Using a Planar Checkerboard Target\n",
    "\n",
    "Accurate metric reasoning from images requires a calibrated camera model that maps three-dimensional world points to two-dimensional image coordinates. In this work, we perform camera calibration using a planar checkerboard target and the standard pinhole camera model with lens distortion, as described in the forward imaging model and calibration framework in the videos.\n",
    "\n",
    "### Calibration Target and Data Acquisition\n",
    "\n",
    "Calibration is performed using a planar checkerboard pattern displayed on a laptop screen. The checkerboard consists of **13 × 10 squares**, which results in **12 × 9 internal corner points**, as required by OpenCV’s checkerboard detection convention that operates on *inner corners* rather than square boundaries.\n",
    "\n",
    "A total of **five images** of the checkerboard are captured using a smartphone (iPhone 13 Pro) camera. The images are taken from different viewing angles and distances to ensure sufficient geometric diversity, which is necessary for stable estimation of both intrinsic and extrinsic parameters. All checkerboard images are assumed to lie on a single plane with known relative geometry.\n",
    "\n",
    "### World Coordinate Definition\n",
    "\n",
    "The world coordinate system is defined on the checkerboard plane, with all points lying on Z_w = 0. The 3D coordinates of the checkerboard corner points are generated assuming a regular grid structure, with unit spacing between adjacent corners. Since the projection matrix is defined only up to scale, absolute units are not required for intrinsic calibration at this stage.\n",
    "\n",
    "### Calibration Procedure\n",
    "\n",
    "For each image:\n",
    "- Checkerboard inner corners are detected using OpenCV’s built in checkerboard corner detection function.\n",
    "- Subpixel refinement is applied to improve localization accuracy.\n",
    "- Corresponding 3D-2D point pairs are accumulated across all images.\n",
    "\n",
    "Calibration quality is evaluated using the mean reprojection error, computed as the root-mean-square pixel distance between observed image points and the projected points obtained using the estimated camera parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66831f2c-40c3-445c-8d6e-1b56d7408861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb32aa69-6367-4ede-bd70-a08284085320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting only inner corners (13x10 to 12x9)\n",
    "checkerboard = (12, 9)\n",
    "\n",
    "# World points on planar calibration target (Z = 0)\n",
    "objp = np.zeros((checkerboard[0] * checkerboard[1], 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:checkerboard[0], 0:checkerboard[1]].T.reshape(-1, 2)\n",
    "\n",
    "# Containers for 3D world points and corresponding 2D image points\n",
    "objpoints = []\n",
    "imgpoints = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d404f044-ca9a-4bea-9575-f2f45d7ea12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration images of checkerboard\n",
    "images = glob.glob(\"calibration_img/image*.jpeg\")\n",
    "\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect checkerboard inner corners using built in opencv function\n",
    "    found, corners = cv2.findChessboardCorners(\n",
    "        gray,\n",
    "        checkerboard,\n",
    "        cv2.CALIB_CB_ADAPTIVE_THRESH +\n",
    "        cv2.CALIB_CB_NORMALIZE_IMAGE\n",
    "    )\n",
    "\n",
    "    if found:\n",
    "        # Subpixel refinement to reduce image point localization error\n",
    "        corners = cv2.cornerSubPix(\n",
    "            gray,\n",
    "            corners,\n",
    "            (11, 11),\n",
    "            (-1, -1),\n",
    "            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "        )\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "# Estimate intrinsic matrix, distortion coefficients, and per-image extrinsics\n",
    "ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
    "    objpoints,\n",
    "    imgpoints,\n",
    "    gray.shape[::-1],\n",
    "    None,\n",
    "    None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b93d685-767e-4684-94e4-dc3214022e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera matrix (K):\n",
      " [[3.14429337e+03 0.00000000e+00 1.44940526e+03]\n",
      " [0.00000000e+00 2.98493024e+03 1.96337804e+03]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Distortion coefficients:\n",
      " [ 1.71646648e-01 -1.83120246e+00 -1.92813775e-02 -3.84685456e-03\n",
      "  3.95203665e+00]\n",
      "Mean reprojection error: 1.7112395295119254\n"
     ]
    }
   ],
   "source": [
    "# Reprojection error computation to evaluate calibration accuracy\n",
    "\n",
    "error = 0\n",
    "points = 0\n",
    "\n",
    "for i in range(len(objpoints)):\n",
    "    projected, _ = cv2.projectPoints(\n",
    "        objpoints[i],\n",
    "        rvecs[i],\n",
    "        tvecs[i],\n",
    "        K,\n",
    "        dist\n",
    "    )\n",
    "    error += cv2.norm(imgpoints[i], projected, cv2.NORM_L2) ** 2\n",
    "    points += len(projected)\n",
    "\n",
    "reprojection_error = np.sqrt(error / points)\n",
    "\n",
    "print(\"Camera matrix (K):\\n\", K)\n",
    "print(\"Distortion coefficients:\\n\", dist.ravel())\n",
    "print(\"Mean reprojection error:\", reprojection_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54c774-b003-4188-9302-4e5d12d19f92",
   "metadata": {},
   "source": [
    "## Part 2 and Part 3: Real-World 2D Measurement and Experimental Validation\n",
    "\n",
    "In this section, we estimate the real-world two-dimensional dimensions of a planar object from a single calibrated image and validate the results using physical measurements.\n",
    "\n",
    "### Measurement Setup and Assumptions\n",
    "\n",
    "A book is used as the test object. The book is placed upright against a wall so that its surface is approximately parallel to the image plane. An image of the book is captured using the same calibrated smartphone camera from a measured distance of approximately 2.2 meters.\n",
    "\n",
    "The following assumptions are made (as per the videos):\n",
    "- All points on the book lie on a single planar surface.\n",
    "- The depth of the object relative to the camera is approximately constant across the object.\n",
    "- The object dimensions are small compared to the camera-to-object distance, allowing magnification to be treated as constant.\n",
    "- Camera intrinsic parameters and lens distortion coefficients obtained in Part 1 are known and fixed.\n",
    "\n",
    "### Real-World Dimension Estimation\n",
    "\n",
    "The captured image is first undistorted using the calibrated camera parameters. Four corner points of the book are then manually selected in pixel coordinates. Using manually selected coordinates is suggested in the videos. \n",
    "\n",
    "Pixel distances between the selected corner points are computed along the width and height of the book. These pixel measurements are converted into real-world dimensions using the calibrated focal lengths and the known camera-to-object distance. This procedure directly follows the image magnification relationship described in the videos, where object size in the image scales linearly with depth.\n",
    "\n",
    "### Experimental Validation\n",
    "\n",
    "To validate the estimated dimensions, the physical size of the book is measured using a ruler. The true book dimensions are 7.6 inches in width and 10 inches in height. These measurements are converted to metric units and compared against the estimated dimensions obtained from the image.\n",
    "\n",
    "Absolute and percentage errors are computed for both width and height. The estimated dimensions are within approximately 10 to 12 percent of the true measurements, which we consider to be reasonably accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8500dad-9eb0-4f4c-b8bf-fe23bde39735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel width: 244.29 px\n",
      "Pixel height: 318.06 px\n",
      "Estimated real-world width: 0.174 m\n",
      "Estimated real-world height: 0.224 m\n"
     ]
    }
   ],
   "source": [
    "# Known camera-to-object distance (meters)\n",
    "Z = 2.2\n",
    "\n",
    "# Load image of object (a book for this notebook)\n",
    "img = cv2.imread(\"detection_img/book.jpg\")\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# Undistort image\n",
    "new_K, _ = cv2.getOptimalNewCameraMatrix(K, dist, (w, h), 1)\n",
    "img_ud = cv2.undistort(img, K, dist, None, new_K)\n",
    "\n",
    "# Manually selected pixel coordinates of book corners\n",
    "# Order: top-left, top-right, bottom-left, bottom-right\n",
    "pts = np.array([\n",
    "    [1404, 2364],\n",
    "    [1648, 2352],\n",
    "    [1410, 2682],\n",
    "    [1664, 2682]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Pixel distances\n",
    "width_px = np.linalg.norm(pts[1] - pts[0])\n",
    "height_px = np.linalg.norm(pts[2] - pts[0])\n",
    "\n",
    "# Intrinsic parameters\n",
    "fx = new_K[0, 0]\n",
    "fy = new_K[1, 1]\n",
    "\n",
    "# Metric dimensions via perspective projection\n",
    "width_m = (Z / fx) * width_px\n",
    "height_m = (Z / fy) * height_px\n",
    "\n",
    "print(f\"Pixel width: {width_px:.2f} px\")\n",
    "print(f\"Pixel height: {height_px:.2f} px\")\n",
    "print(f\"Estimated real-world width: {width_m:.3f} m\")\n",
    "print(f\"Estimated real-world height: {height_m:.3f} m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f055483-3665-4738-8337-75134cdeedef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
